{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3a17b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a836a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(dict_network, seed = 99):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    for dict_layer in dict_network:        \n",
    "        dim_layer_in = dict_layer['dim_in']\n",
    "        dim_layer_out = dict_layer['dim_out']  \n",
    "        dict_layer['W'] = np.random.uniform(-1, 1, (dim_layer_out, dim_layer_in))\n",
    "        dict_layer['B'] = np.random.uniform(-1, 1, (dim_layer_out, 1))\n",
    "        \n",
    "    return dict_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9277387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(input_, dict_network):\n",
    "    Z = input_\n",
    "    \n",
    "    for dict_layer in dict_network:\n",
    "\n",
    "        activation = dict_layer['activation']\n",
    "        activation_params = dict_layer['activation_params']\n",
    "        W = dict_layer['W']\n",
    "        B = dict_layer['B'].T.flatten()\n",
    "    \n",
    "        X = Z\n",
    "        dict_layer['X'] = X\n",
    "        \n",
    "        V = np.dot(W, X) + B\n",
    "\n",
    "        if len(activation_params) == 0:\n",
    "            raise Exception('Activation parameter is empty.')   \n",
    "            \n",
    "        elif activation == 'logi':\n",
    "            a = activation_params[0]\n",
    "            Z = 1 / (1 + np.exp(-a*V))\n",
    "            \n",
    "        elif activation == 'tanh':\n",
    "            a = activation_params[0]\n",
    "            c = activation_params[1]\n",
    "            Z = a * np.tanh(c*V)\n",
    "            \n",
    "        elif activation == 'relu':\n",
    "            a = activation_params[0]\n",
    "            Z = np.maximum(V,a*V)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('Activation function is not supported')\n",
    "            \n",
    "        dict_layer['Z'] = Z\n",
    "        dict_layer['V'] = V\n",
    "\n",
    "    return dict_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0dc5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(output_data, dict_network, learning_rate=0.85):\n",
    "    \n",
    "    d = output_data    \n",
    "    o = dict_network[-1]['Z']\n",
    "    error = d - o\n",
    "    \n",
    "    for index, dict_layer in reversed(list(enumerate(dict_network))):\n",
    "        \n",
    "        activation = dict_layer['activation']\n",
    "        activation_params = dict_layer['activation_params']\n",
    "        Z = dict_layer['Z']\n",
    "        X = dict_layer['X']\n",
    "        V = dict_layer['V']\n",
    "        W = dict_layer['W']\n",
    "        \n",
    "        if len(activation_params) == 0:\n",
    "            raise Exception('Activation parameter is empty.')      \n",
    "            \n",
    "        elif activation == 'logi':\n",
    "            a = activation_params[0]\n",
    "            dZ =  a * error * Z * (1-Z)\n",
    "            \n",
    "        elif activation == 'tanh':\n",
    "            a = activation_params[0]\n",
    "            c = activation_params[1]\n",
    "            dZ = (c / a) * error * (a - Z) * (a + Z)\n",
    "            \n",
    "        elif activation == 'relu':\n",
    "            a = activation_params[0]\n",
    "            mult = np.where(V > 0, 1, a)\n",
    "            dZ = mult * error\n",
    "            \n",
    "        else:\n",
    "            raise Exception('Activation function is not supported')\n",
    "        \n",
    "        dict_layer['dZ'] = dZ\n",
    "        dW = learning_rate * np.outer(dZ, X)\n",
    "        dict_layer['dW'] = dW\n",
    "        dB = learning_rate * dZ\n",
    "        dict_layer['dB'] = dB\n",
    "        error =  np.dot(dZ, W) \n",
    "        \n",
    "    return dict_network, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0855d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(init_dict_network, update_dict_network, delta_weights, delta_biases, momentum):\n",
    "    \n",
    "    for index, dict_layer in enumerate(update_dict_network):\n",
    "        \n",
    "        curr_weight = dict_layer['weight']\n",
    "        curr_bias = dict_layer['bias']\n",
    "        \n",
    "        prev_delta_weight = dict_layer['prev_delta_weight']\n",
    "        prev_delta_bias = dict_layer['prev_delta_bias']\n",
    "        \n",
    "        delta_weight = np.array(delta_weights[index])\n",
    "        delta_bias = np.array(delta_biases[index])\n",
    "        delta_bias = delta_bias.reshape(-1,1)\n",
    "\n",
    "        update_bias = curr_bias + momentum * prev_delta_bias + delta_bias\n",
    "        update_weight = curr_weight + momentum * prev_delta_weight + delta_weight\n",
    "\n",
    "        init_dict_network[index]['weight'] = update_weight\n",
    "        init_dict_network[index]['bias'] = update_bias\n",
    "        \n",
    "        init_dict_network[index]['prev_delta_weight'] = delta_weight\n",
    "        init_dict_network[index]['prev_delta_bias'] = delta_bias\n",
    "        \n",
    "    return init_dict_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06406568",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!!!YOU CAN INPUT ANY NUMBER OF NODES AND LAYERS HERE!!!\n",
    "!!!   JUST ADD A DICTIONARY FOR A LAYER SPECIFICS   !!!\n",
    "!!!   CHANGE DIM_IN FOR NO OF NODES IN PREV LAYER   !!!\n",
    "!!!   CHANGE DIM_OUT FOR NO OF NODES IN CUR LAYER   !!!\n",
    "'''\n",
    "\n",
    "input1 = [0.4, 0.5, -0.7]\n",
    "output1 = [0.83, 0.74]\n",
    "\n",
    "dict_network_tanh = [\n",
    "    {'dim_in': 3, 'dim_out': 4, 'activation': 'tanh', 'activation_params': [1.716, 2/3]},\n",
    "    {'dim_in': 4, 'dim_out': 3, 'activation': 'tanh', 'activation_params': [1.716, 2/3]},\n",
    "    {'dim_in': 3, 'dim_out': 2, 'activation': 'tanh', 'activation_params': [1.716, 2/3]}\n",
    "]\n",
    "\n",
    "dict_network_relu = [\n",
    "    {'dim_in': 3, 'dim_out': 4, 'activation': 'relu', 'activation_params': [0.01]},\n",
    "    {'dim_in': 4, 'dim_out': 3, 'activation': 'relu', 'activation_params': [0.01]},\n",
    "    {'dim_in': 3, 'dim_out': 2, 'activation': 'relu', 'activation_params': [0.01]}\n",
    "]\n",
    "\n",
    "dict_network_logi = [\n",
    "    {'dim_in': 3, 'dim_out': 4, 'activation': 'logi', 'activation_params': [0.7]},\n",
    "    {'dim_in': 4, 'dim_out': 3, 'activation': 'logi', 'activation_params': [0.7]},\n",
    "    {'dim_in': 3, 'dim_out': 2, 'activation': 'logi', 'activation_params': [0.7]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c3758f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.24147125e-05  1.51269444e-05 -1.81116624e-05]\n"
     ]
    }
   ],
   "source": [
    "dict_network = dict_network_relu\n",
    "dict_network = initialize(dict_network)\n",
    "dict_network = forward_propagation(input1, dict_network)\n",
    "dict_network, error = backward_propagation(output1, dict_network)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fea1eb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK ARCHITECTURE\n",
      "\n",
      "LAYER 1\n",
      "\n",
      "Input: \n",
      "[0.4, 0.5, -0.7]\n",
      "Init Weight: \n",
      "[[ 0.34455712 -0.0238432   0.65099035]\n",
      " [-0.93710722  0.61609993  0.13123484]\n",
      " [-0.404755   -0.90660856  0.9812548 ]\n",
      " [-0.98634853  0.53958606  0.4935342 ]]\n",
      "Init Bias: \n",
      "[[-0.24512213]\n",
      " [-0.0117051 ]\n",
      " [ 0.85789678]\n",
      " [-0.20909191]]\n",
      "V: \n",
      "[-0.57491412 -0.17036241 -0.44418786 -0.67931224]\n",
      "Output: \n",
      "[-0.00574914 -0.00170362 -0.00444188 -0.00679312]\n",
      "\n",
      "Delta: \n",
      "[-7.66078949e-05  3.09663281e-05  1.82162721e-05  1.98986660e-05]\n",
      "New Weight: \n",
      "[[-2.60466843e-05 -3.25583553e-05  4.55816975e-05]\n",
      " [ 1.05285516e-05  1.31606894e-05 -1.84249652e-05]\n",
      " [ 6.19353252e-06  7.74191565e-06 -1.08386819e-05]\n",
      " [ 6.76554643e-06  8.45693303e-06 -1.18397062e-05]]\n",
      "New Bias: \n",
      "[-6.51167107e-05  2.63213789e-05  1.54838313e-05  1.69138661e-05]\n",
      "\n",
      "LAYER 2\n",
      "\n",
      "Input: \n",
      "[-0.00574914 -0.00170362 -0.00444188 -0.00679312]\n",
      "Init Weight: \n",
      "[[ 0.94791259  0.04882943 -0.81277381  0.62661683]\n",
      " [-0.57662643  0.10869157 -0.41546177  0.63228472]\n",
      " [ 0.65608513 -0.55684526  0.2896694  -0.80963676]]\n",
      "Init Bias: \n",
      "[[-0.17667352]\n",
      " [-0.80626948]\n",
      " [-0.711978  ]]\n",
      "V: \n",
      "[-0.18285283 -0.8055893  -0.71058799]\n",
      "Output: \n",
      "[-0.00182853 -0.00805589 -0.00710588]\n",
      "\n",
      "Delta: \n",
      "[-0.004173   -0.00040895 -0.00600678]\n",
      "New Weight: \n",
      "[[2.03924952e-05 6.04284096e-06 1.57555682e-05 2.40955492e-05]\n",
      " [1.99843138e-06 5.92188592e-07 1.54402008e-06 2.36132465e-06]\n",
      " [2.93537547e-05 8.69830148e-06 2.26791808e-05 3.46840753e-05]]\n",
      "New Bias: \n",
      "[-0.00354705 -0.00034761 -0.00510576]\n",
      "\n",
      "LAYER 3\n",
      "\n",
      "Input: \n",
      "[-0.00182853 -0.00805589 -0.00710588]\n",
      "Init Weight: \n",
      "[[-0.57560765 -0.04668769 -0.84477233]\n",
      " [-0.52991244 -0.98689399  0.79728837]]\n",
      "Init Bias: \n",
      "[[ 0.10446885]\n",
      " [-0.66490674]]\n",
      "V: \n",
      "[ 0.11190033 -0.66165291]\n",
      "Output: \n",
      "[ 0.11190033 -0.00661653]\n",
      "\n",
      "Delta: \n",
      "[0.71809967 0.00746617]\n",
      "New Weight: \n",
      "[[-1.11610576e-03 -4.91719397e-03 -4.33732050e-03]\n",
      " [-1.16042807e-05 -5.11246342e-05 -4.50956227e-05]]\n",
      "New Bias: \n",
      "[0.61038472 0.00634624]\n"
     ]
    }
   ],
   "source": [
    "print('NETWORK ARCHITECTURE')\n",
    "i = 0\n",
    "for dict_layer in dict_network:\n",
    "    i += 1\n",
    "    print(f\"\\nLAYER {i}\")\n",
    "    print(f\"\\nInput: \\n{dict_layer['X']}\")\n",
    "    print(f\"Init Weight: \\n{dict_layer['W']}\")\n",
    "    print(f\"Init Bias: \\n{dict_layer['B']}\")\n",
    "    print(f\"V: \\n{dict_layer['V']}\")\n",
    "    print(f\"Output: \\n{dict_layer['Z']}\\n\")\n",
    "    \n",
    "    print(f\"Delta: \\n{dict_layer['dZ']}\")\n",
    "    print(f\"New Weight: \\n{dict_layer['dW']}\")\n",
    "    print(f\"New Bias: \\n{dict_layer['dB']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5929ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
